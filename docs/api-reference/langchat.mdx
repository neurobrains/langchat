---
title: LangChat
description: Main LangChat class API reference for creating conversational AI applications
---

import { ParamField } from 'mintlify';
import { Callout } from 'mintlify';

## Class: LangChat

The primary interface for using LangChat. Simplifies initialization and chat operations.

### Constructor

```python
LangChat(config: Optional[LangChatConfig] = None)
```

Creates a new LangChat instance.

**Parameters:**

<ParamField path="config" type="LangChatConfig | None" default="None">
  LangChat configuration. If None, creates config from environment variables using `LangChatConfig.from_env()`.
</ParamField>

**Example:**

```python
from langchat import LangChat, LangChatConfig

# With custom config
config = LangChatConfig(
    openai_api_keys=["sk-..."],
    pinecone_api_key="pcsk-...",
    pinecone_index_path="my-index",
    supabase_url="https://xxxxx.supabase.co",
    supabase_key="eyJ..."
)
langchat = LangChat(config=config)

# Or from environment variables
langchat = LangChat()  # Uses LangChatConfig.from_env()
```

<Callout type="info">
All adapters are automatically initialized when LangChat is created.
</Callout>

### Methods

#### `async chat()`

Process a chat query asynchronously.

```python
async def chat(
    self,
    query: str,
    user_id: str,
    domain: str = "default"
) -> dict
```

**Parameters:**

<ParamField path="query" type="str" required>
  User query text
</ParamField>

<ParamField path="user_id" type="str" required>
  User identifier. Used for session management and chat history.
</ParamField>

<ParamField path="domain" type="str" default="default">
  User domain. Used to separate different conversation contexts (e.g., "education", "travel", "support").
</ParamField>

**Returns:**

`dict` - Dictionary with the following keys:
- `response` (str): AI response text
- `user_id` (str): User identifier
- `timestamp` (str): ISO format timestamp
- `status` (str): "success" or "error"
- `response_time` (float): Response time in seconds
- `error` (str, optional): Error message if status is "error"

**Example:**

```python
import asyncio
from langchat import LangChat, LangChatConfig

async def main():
    config = LangChatConfig.from_env()
    langchat = LangChat(config=config)
    
    # Response is automatically displayed in a Rich panel box
    result = await langchat.chat(
        query="What are the best universities in Europe?",
        user_id="user123",
        domain="education"
    )
    
    # Response already shown in panel above
    # Access response data programmatically if needed:
    print(f"Status: {result['status']}")
    print(f"Response Time: {result['response_time']:.2f}s")

asyncio.run(main())
```

**What happens under the hood:**

1. Gets or creates user session
2. Generates standalone question from query and chat history
3. Retrieves relevant documents from Pinecone
4. Reranks documents using Flashrank
5. Generates response using OpenAI with context
6. Saves chat history to Supabase
7. Updates in-memory session
8. Returns response with metadata

#### `chat_sync()`

Synchronous version of `chat()` method.

```python
def chat_sync(
    self,
    query: str,
    user_id: str,
    domain: str = "default"
) -> dict
```

**Parameters:**

<ParamField path="query" type="str" required>
  User query text
</ParamField>

<ParamField path="user_id" type="str" required>
  User identifier
</ParamField>

<ParamField path="domain" type="str" default="default">
  User domain
</ParamField>

**Returns:**

`dict` - Same as `chat()` method

**Example:**

```python
from langchat import LangChat, LangChatConfig

config = LangChatConfig.from_env()
langchat = LangChat(config=config)

result = langchat.chat_sync(
    query="Hello!",
    user_id="user123",
    domain="general"
)

print(result["response"])
```

<Callout type="warning">
For production, prefer `chat()` over `chat_sync()` as it's non-blocking and more efficient.
</Callout>

#### `get_session()`

Get or create a user session.

```python
def get_session(
    self,
    user_id: str,
    domain: str = "default"
) -> UserSession
```

**Parameters:**

<ParamField path="user_id" type="str" required>
  User identifier
</ParamField>

<ParamField path="domain" type="str" default="default">
  User domain
</ParamField>

**Returns:**

`UserSession` - User session instance with chat history and metadata

**Example:**

```python
from langchat import LangChat, LangChatConfig

config = LangChatConfig.from_env()
langchat = LangChat(config=config)

# Get or create session
session = langchat.get_session(
    user_id="user123",
    domain="education"
)

# Access chat history
print(session.chat_history)  # List of (query, response) tuples
print(session.domain)  # "education"
print(session.user_id)  # "user123"
```

#### `load_and_index_documents()`

Load documents from a file, split them into chunks, and index them to Pinecone. Prevents duplicate documents from being indexed multiple times.

```python
def load_and_index_documents(
    self,
    file_path: str,
    chunk_size: int = 1000,
    chunk_overlap: int = 200,
    namespace: Optional[str] = None,
    prevent_duplicates: bool = True
) -> dict
```

**Parameters:**

<ParamField path="file_path" type="str" required>
  Path to the document file (supports PDF, TXT, CSV, etc.)
</ParamField>

<ParamField path="chunk_size" type="int" default="1000">
  Size of each text chunk in characters
</ParamField>

<ParamField path="chunk_overlap" type="int" default="200">
  Overlap between chunks in characters
</ParamField>

<ParamField path="namespace" type="str | None" default="None">
  Optional Pinecone namespace to store documents in
</ParamField>

<ParamField path="prevent_duplicates" type="bool" default="True">
  If True, checks for existing documents before adding to prevent duplicates
</ParamField>

**Returns:**

`dict` - Dictionary with indexing results:
- `status` (str): "success"
- `chunks_indexed` (int): Number of chunks indexed
- `chunks_skipped` (int): Number of duplicate chunks skipped
- `documents_loaded` (int): Number of documents loaded
- `file_path` (str): Path to indexed file
- `namespace` (str | None): Namespace used

**Example:**

```python
from langchat import LangChat, LangChatConfig

config = LangChatConfig.from_env()
langchat = LangChat(config=config)

# Index a document
result = langchat.load_and_index_documents(
    file_path="document.pdf",
    chunk_size=1000,
    chunk_overlap=200
)

print(f"Indexed {result['chunks_indexed']} chunks")
print(f"Skipped {result.get('chunks_skipped', 0)} duplicates")
```

#### `load_and_index_multiple_documents()`

Load multiple documents, split them, and index them to Pinecone in batch.

```python
def load_and_index_multiple_documents(
    self,
    file_paths: List[str],
    chunk_size: int = 1000,
    chunk_overlap: int = 200,
    namespace: Optional[str] = None,
    prevent_duplicates: bool = True
) -> dict
```

**Parameters:**

<ParamField path="file_paths" type="List[str]" required>
  List of file paths to load and index
</ParamField>

<ParamField path="chunk_size" type="int" default="1000">
  Size of each text chunk
</ParamField>

<ParamField path="chunk_overlap" type="int" default="200">
  Overlap between chunks
</ParamField>

<ParamField path="namespace" type="str | None" default="None">
  Optional Pinecone namespace
</ParamField>

<ParamField path="prevent_duplicates" type="bool" default="True">
  Prevent duplicate indexing
</ParamField>

**Returns:**

`dict` - Dictionary with batch results:
- `status` (str): "completed"
- `total_chunks_indexed` (int): Total chunks indexed
- `total_chunks_skipped` (int): Total duplicates skipped
- `files_processed` (int): Total files processed
- `files_succeeded` (int): Files successfully indexed
- `files_failed` (int): Files that failed
- `results` (List[Dict]): Detailed results per file
- `errors` (List[Dict] | None): List of errors

**Example:**

```python
result = langchat.load_and_index_multiple_documents(
    file_paths=["doc1.pdf", "doc2.txt", "doc3.csv"],
    chunk_size=1000,
    chunk_overlap=200
)

print(f"Total chunks: {result['total_chunks_indexed']}")
print(f"Files succeeded: {result['files_succeeded']}")
```

### Properties

#### `config`

Access the configuration object.

```python
config: LangChatConfig
```

**Example:**

```python
langchat = LangChat(config=my_config)
print(langchat.config.openai_model)  # "gpt-4o-mini"
print(langchat.config.server_port)  # 8000
```

#### `engine`

Access the underlying engine.

```python
engine: LangChatEngine
```

**Example:**

```python
# Access engine directly (advanced)
engine = langchat.engine
print(engine.llm)  # OpenAILLMService instance
print(engine.vector_adapter)  # PineconeVectorAdapter instance
```

## Usage Examples

### Basic Usage

```python
import asyncio
from langchat import LangChat, LangChatConfig

async def main():
    config = LangChatConfig.from_env()
    langchat = LangChat(config=config)
    
    result = await langchat.chat(
        query="What can you help me with?",
        user_id="user123",
        domain="general"
    )
    
    print(result["response"])

asyncio.run(main())
```

### With Custom Configuration

```python
from langchat import LangChat, LangChatConfig

config = LangChatConfig(
    openai_api_keys=["sk-key1", "sk-key2"],
    openai_model="gpt-4o",
    pinecone_api_key="pcsk-...",
    pinecone_index_path="my-index",
    supabase_url="https://xxxxx.supabase.co",
    supabase_key="eyJ...",
    retrieval_k=10,
    reranker_top_n=5
)

langchat = LangChat(config=config)
```

### Multi-Domain Sessions

```python
async def handle_multiple_domains():
    langchat = LangChat(config=config)
    
    # Education domain
    result1 = await langchat.chat(
        query="What universities offer CS?",
        user_id="user123",
        domain="education"
    )
    
    # Travel domain (separate session)
    result2 = await langchat.chat(
        query="Best travel destinations?",
        user_id="user123",
        domain="travel"
    )
```

### Error Handling

```python
async def chat_with_error_handling():
    langchat = LangChat(config=config)
    
    try:
        result = await langchat.chat(
            query="Hello!",
            user_id="user123",
            domain="general"
        )
        
        if result["status"] == "success":
            print(f"Response: {result['response']}")
        else:
            print(f"Error: {result.get('error', 'Unknown error')}")
            
    except Exception as e:
        print(f"Exception: {str(e)}")
```

### Document Indexing

```python
async def index_documents():
    langchat = LangChat(config=config)
    
    # Index single document
    result = langchat.load_and_index_documents(
        file_path="document.pdf",
        chunk_size=1000,
        chunk_overlap=200,
        namespace="company-docs"
    )
    
    # Index multiple documents
    result = langchat.load_and_index_multiple_documents(
        file_paths=["doc1.pdf", "doc2.txt"],
        chunk_size=1000
    )
```

## Related Documentation

- **[LangChatConfig](/api-reference/langchat-config)** - Configuration API
- **[LangChatEngine](/api-reference/langchat-engine)** - Engine API
- **[UserSession](/api-reference/user-session)** - Session API
- **[DocumentIndexer](/api-reference/document-indexer)** - Standalone document indexing API
- **[Document Indexing Guide](/guides/document-indexing)** - Complete guide to document indexing
- **[Quick Start](/quick-start)** - Quick start guide
- **[Examples](/examples/basic-usage)** - Usage examples

<Callout type="info">
[![Discord](https://i.postimg.cc/prp9gdxq/Chat-GPT-Image-Nov-14-2025-05-55-47-PM.png)](https://discord.gg/v9rqMDR2) **Need help? Reach out!** Join our [Discord community](https://discord.gg/v9rqMDR2) for support and discussions.
</Callout>

